{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmvFilQfrd2M"
   },
   "source": [
    "# <font color=\"maroon\"> NLP Toolkits and Preprocessing Techniques </font>\n",
    "Python libraries for natural language processing\n",
    "1. Converting text to a meaningful format for analysis\n",
    "2. Preprocessing and cleaning text\n",
    "\n",
    "Open-Source Libraries<br> \n",
    "1. <font color=\"red\">NLTK<br> </font>\n",
    "2. <font color=\"red\">TextBlob<br></font>\n",
    "3. SpaCy<br>\n",
    "4. GenSim<br>\n",
    "\n",
    "Cloud-Based NLP Services<br> \n",
    "1. IBM Watson<br>\n",
    "2. Google Cloud Natural Language API\n",
    "3. Amazon Comprehend\n",
    "4. Microsoft Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaWPMOOxrd2T"
   },
   "source": [
    "## How to Install NLTK?\n",
    "\n",
    "### Method (i) Command Line\n",
    "pip install nltk<br>\n",
    "import nltk<br>\n",
    "nltk.download()\n",
    "\n",
    "### Method (ii) Anaconda Navigator (Environment)\n",
    "![Installation of NLTK library](NLTK.png)\n",
    "\n",
    "### Method (iii) Download Package and Place into Site-package directory\n",
    "Install nltk toolkit from https://sourceforge.net/projects/nltk/<br>\n",
    "![Installation of NLTK library](nltk_package.png)\n",
    "<br>Locate the package into site-package directory <br>\n",
    "(to find the path:<br> import site <br>site.getsitepackages())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfqK8WmIrd2U",
    "outputId": "5a7e396d-aa80-4619-9948-ea9991e66dc8"
   },
   "outputs": [],
   "source": [
    "import site\n",
    "site.getsitepackages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9fuAB41rd2W"
   },
   "source": [
    "## Sample Text Data\n",
    "\n",
    "Consider this sentence:\n",
    "<br>**Hi Mr. Smith! I am going to buy some vegetables (3 tomatoes and 3 cucumbers) from the\n",
    "store. Should I pick up some black-eyed peas as well?**\n",
    "\n",
    "Text data is messy and unstructured. To analyze this data, we need to preprocess the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gctL6qcrd2Y"
   },
   "source": [
    "![](https://i.imgur.com/pt5p6Hb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJGrFa0qrd2Y"
   },
   "source": [
    "# Code: Tokenization (Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_xmMSXArd2Y",
    "outputId": "86d2e0b1-a63d-4a0a-8300-71e2b5386ca8"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "my_text = '''Hi Mr. Smith! I am going to buy some vegetables (3 tomatoes and 3 cucumbers)\n",
    "from the store. Should I pick up some black-eyed peas as well?'''\n",
    "\n",
    "print(word_tokenize(my_text)) # print function requires Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbzU3hrZrd2Z"
   },
   "source": [
    "# Code: Tokenization (Sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEFjou9Mrd2a",
    "outputId": "7074e48b-4a57-4a9c-dd6a-9a8fe0c723b5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "my_text = '''Hi Mr. Smith! I am going to buy some vegetables (3 tomatoes and 3 cucumbers)\n",
    "from the store. Should I pick up some black-eyed peas as well?'''\n",
    "\n",
    "print(sent_tokenize(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOhJjUDlrd2e"
   },
   "source": [
    "![](https://i.imgur.com/3L6x92C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr8o5eNmrd2e"
   },
   "source": [
    "# Code: Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgqeFhzerd2e",
    "outputId": "11b48404-b14f-4c57-e331-7a3692558eab"
   },
   "outputs": [],
   "source": [
    "import re # Regular expression library\n",
    "import string\n",
    "\n",
    "#Replace punctuations with a white space\n",
    "s = re.sub('[^\\w\\s]','',my_text)\n",
    "s\n",
    "\n",
    "#OR\n",
    "clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)  # string.punctuation is a string defined in the string module of Python. It contains all the punctuation characters: !\"#$%&'()*+,-./:;<=>?@[\\]^_{|}~`.\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rwv4BKQzrd2f"
   },
   "source": [
    "# Code: Make All Text Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evdfIo5Ird2g",
    "outputId": "7481428f-e34f-40b2-b0d4-72f0f3b13bad"
   },
   "outputs": [],
   "source": [
    "clean_text = s.lower()\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DEi4_gYrd2h"
   },
   "source": [
    "# Code: Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAuSuRNBrd2h",
    "outputId": "cf2bf5b2-e772-4378-842e-6fb4fea3221b"
   },
   "outputs": [],
   "source": [
    "# Removes all words containing digits\n",
    "clean_text = re.sub('\\d', '', clean_text)  #\\d is a special sequence in regular expressions that matches any digit (0-9).\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAyxd4Kgrd2i"
   },
   "source": [
    "# <font color='blue'>Preprocessing: Stop Words</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AI9336Hqrd2i"
   },
   "source": [
    "![](https://i.imgur.com/T5RJXrX.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gg_vlZdErd4Z"
   },
   "source": [
    "# Code: Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwAJ5PK1rd4Z",
    "outputId": "6907c5a8-2fa9-47c2-8b29-14311522f550"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBWIOjUCrd4a"
   },
   "source": [
    "# Code: Remove Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKfQc-qfrd4a"
   },
   "source": [
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">CountVectorizer</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fAaaw_Zrd4b",
    "outputId": "0be39ff8-d892-4e00-f736-3ea100b4d434"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "my_text = [\"Hi Mr. Smith! Iâ€™m going to buy some vegetables \\\n",
    "(3 tomatoes and 3 cucumbers from the store. Should I pick up some black-eyed peas as well?\"]\n",
    "\n",
    "# Incorporate stop words when creating the count vectorizer\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(my_text)  # this equavalent to X=CountVectorizer(stop_words='english').fit_transform(my_text)\n",
    "print (X)\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())\n",
    "\n",
    "# Reference: https://www.geeksforgeeks.org/difference-between-pandas-vs-numpy/\n",
    "# self learn pandas: https://www.w3schools.com/python/pandas/pandas_intro.asp\n",
    "# self learn numpy: https://www.w3schools.com/python/numpy/numpy_intro.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgPbENbQrd4b"
   },
   "source": [
    "The process of using CountVectorizer.fit_transform involves the following steps:\n",
    "\n",
    "(1)Tokenization: The text documents are first tokenized, breaking them into individual words or tokens.\n",
    "\n",
    "(2)Vocabulary Building (fit): CountVectorizer builds a vocabulary, which is a dictionary mapping each unique word (or token) in the documents to an integer index.\n",
    "\n",
    "(3)Counting (transform): It then counts the occurrences of each word in each document and stores these counts in a sparse matrix, where rows represent documents, and columns represent the vocabulary words. Each element of the matrix represents the frequency of the corresponding word in the respective document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OMyEkOQrd4c"
   },
   "source": [
    "![](https://i.imgur.com/9qllh8j.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlkukpcRrd4c"
   },
   "source": [
    "# Code: Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUzdKeTard4c",
    "outputId": "d8984f4c-08a3-4d94-b8a7-37cd4ebd448a"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer  #more stem library: https://www.nltk.org/api/nltk.stem.html\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# Try some stems\n",
    "print('drive:{}'.format(stemmer.stem('drive')))\n",
    "print('drives:{}'.format(stemmer.stem('drives')))\n",
    "print('driver:{}'.format(stemmer.stem('driver')))\n",
    "print('drivers:{}'.format(stemmer.stem('drivers')))\n",
    "print('driven:{}'.format(stemmer.stem('driven')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvJnHx7Krd4c"
   },
   "source": [
    "# Code: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMCMR9c-rd4d",
    "outputId": "4bfc4dfe-2c85-459d-83e9-8d99dbab59b1"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer  # Reference: https://www.nltk.org/api/nltk.stem.wordnet.html\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "input_str = \"been had done languages cities mice\"\n",
    "input_str = word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0kvAH-Lrd4d"
   },
   "source": [
    "![](https://i.imgur.com/8edVsCR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHDjq6x_rd4e"
   },
   "source": [
    "# Code: Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 898,
     "status": "ok",
     "timestamp": 1691373190394,
     "user": {
      "displayName": "CHING PANG GOH",
      "userId": "03150219032412111071"
     },
     "user_tz": -480
    },
    "id": "yU1uQFs1rd4e",
    "outputId": "586674c3-6ab5-4d4b-9e91-1b8f5580a107"
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "my_text2 = \"James Smith is having a live band in the United States.\"\n",
    "tokens = pos_tag(word_tokenize(my_text))\n",
    "tokens2 = pos_tag(word_tokenize(my_text2))\n",
    "print(\"Sentence 1:\",tokens)\n",
    "print(\"Sentence 2:\",tokens2)\n",
    "\n",
    "#Reference:https://pythonspot.com/nltk-speech-tagging/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8vfPiB0Erd4f"
   },
   "source": [
    "![POS](nltk-speech-codes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfpmNQBHrd4f"
   },
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "error",
     "timestamp": 1691373215467,
     "user": {
      "displayName": "CHING PANG GOH",
      "userId": "03150219032412111071"
     },
     "user_tz": -480
    },
    "id": "-clqK5cCrd4f",
    "outputId": "8939e637-ccc7-4dd7-d7f1-b99ba28b2414"
   },
   "outputs": [],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk.chunk import ne_chunk  #Spacy reference: https://medium.com/geekculture/named-entity-recognition-ner-part-ii-implementation-with-open-source-packages-2713c4c4a8c5\n",
    "\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "tokens = pos_tag(word_tokenize(my_text)) # this labels each word as a part of speech\n",
    "entities = ne_chunk(tokens) # this extracts entities from the list of words\n",
    "entities.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjiixNgFrd4f"
   },
   "source": [
    "# <font color=\"blue\"> Prepocessing: Compound Term Extraction </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXsIaKOxrd4h"
   },
   "source": [
    "![](https://i.imgur.com/q1WuWai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4KwWt67rd4h"
   },
   "source": [
    "# Code: Compound Term Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pn3S4RwHrd4h"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer #https://www.nltk.org/api/nltk.tokenize.mwe.html\n",
    "\n",
    "my_text = \"You all are the greatest students of all time.\"\n",
    "\n",
    "mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
    "\n",
    "mwe_tokens\n",
    "\n",
    "# New York City, take into account, make use of, high probability, kick the bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbPyyQ_7rd4u"
   },
   "outputs": [],
   "source": [
    "# Basic example, https://www.w3schools.com/python/python_lambda.asp\n",
    "square_me=lambda x: x*x\n",
    "\n",
    "my_numbers=[9, 3, 4, 100, 2, 1]\n",
    "my_numbers_squared = list(map(square_me, my_numbers)) #map = applies a function to all the items in an input_list\n",
    "                                                      #map(function, iterable)\n",
    "print(my_numbers_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1rrNgqErd4u"
   },
   "source": [
    "# <font color=red>Preprocessing Exercise </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G37XzEBLrd4u"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "We will be using review data from Kaggle to practice preprocessing text data. The dataset contains user reviews for many products, but today we'll be focusing on the product in the dataset that had the most reviews - an oatmeal cookie.\n",
    "\n",
    "The following code will help you load in the data. If this is your first time using nltk, you'll to need to pip install it first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UN4M0O6krd4v"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download() <-- Run this if it's your first time using nltk to download all of the datasets and models\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7wE2LEHrd4v"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('cookie_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-TQNFEqrd4w"
   },
   "source": [
    "**Question 1:**\n",
    "\n",
    "Determine how many reviews there are in total.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VNVnbS1rd4w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2SHURS_rd4x"
   },
   "source": [
    "**Question 2:**\n",
    "    \n",
    "Determine the percentage of 1, 2, 3, 4 and 5 star reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mrh7DCk4rd4y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHZ7MVt-rd4y"
   },
   "source": [
    "**Question 3:**\n",
    "\n",
    "(a) Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beCkKopYrd4z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. df['reviews'] refers to the 'reviews' column in your DataFrame df \n",
    "2. .apply(lambda x: ...) is used to apply a function (defined by the lambda function) along the axis of the DataFrame.\n",
    "3. lambda x: ' '.join([word for word in x.split() if word not in (stop)]) is a lambda function that:\n",
    "   <br>a. Splits each review x into a list of words (x.split()).\n",
    "   <br>b. Iterates through each word in this list (for word in x.split()).\n",
    "   <br>c. Checks if each word is not in the stop list (i.e., if it's not a stopword).\n",
    "   <br>d. If the word is not a stopword, it includes it in the list comprehension ([word for word in x.split() if word not in (stop)]).\n",
    "   <br>e. Joins these words back into a single string with spaces separating them (' '.join(...))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pr4R-PKWrd4z"
   },
   "source": [
    "(b) Change to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LopdQ216rd40"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00e5zCM5rd40"
   },
   "source": [
    "(b) Perform stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8h_Oeeuwrd41"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Constructs a new list (documents) by iterating over each element (x) in the list l_case.\n",
    "2. For each document i in l_case, the inner list comprehension splits i into words using i.split(\" \").\n",
    "3. It then applies stemming to each word using sno.stem(word), where sno is an object or function that performs stemming.\n",
    "4. The outer comprehension gathers these lists of stemmed words (one list per document) and constructs a new list (documents) where each element corresponds to a document from l_case, but with each word stemmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKTiJ341rd43"
   },
   "source": [
    "# TextBlob\n",
    "\n",
    "### Another toolkit other than NLTK\n",
    "\n",
    "- Wraps around NLTK and makes it easier to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtSKm8Jmrd44"
   },
   "source": [
    "# TextBlob Demo: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install textblob  #Install the library before importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3sm_gcqLrd44",
    "outputId": "7e70c116-f4e5-476b-eaa4-5189bd40030e"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "my_text = TextBlob(\"We're moving from NLTK to TextBlob. How fun!\")\n",
    "my_text.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vz6kz6o6rd44"
   },
   "source": [
    "# TextBlob Demo: Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7x_TFgdrd45",
    "outputId": "4cc6163b-4620-458c-ac30-fb960c2070bb"
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(\"I'm graat at speling.\")\n",
    "print(blob.correct()) # print function requires Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvfqZ_ulrd45"
   },
   "source": [
    "<font color=\"blue\">\n",
    "## How does the correct function work?  <br>\n",
    "    \n",
    "- Calculates the Levenshtein distance between the word â€˜graatâ€™ and all words in its word list </br>\n",
    "- Of the words with the smallest Levenshtein distance, it outputs the most popular word </br></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNRUkzwyrd46"
   },
   "source": [
    "# TextBlob Demo: Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVr6l2Bkrd46",
    "outputId": "1261db72-7693-4a10-f107-90ed5be00e23"
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(\"John hits the ball.\")\n",
    "for words, tag in blob.tags:\n",
    "    print (words, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRWBpLVfrd46"
   },
   "source": [
    "# TextBlob Demo: Language Detection and Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uimeap4prd47",
    "outputId": "d29280aa-2362-4af1-ad56-c37a4a404d5d"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"This is a sample text in English.\"\n",
    "blob = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-9hqISZrd47",
    "outputId": "f57af290-ef95-4346-d9be-ad08e7ad7709"
   },
   "outputs": [],
   "source": [
    "!pip install langdetect  #install the library before importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6j9k5_p1rd47",
    "outputId": "a4d55685-6737-4450-ed57-f86597de77f5"
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "text = \"This is a sample text in English.\"\n",
    "language = detect(text)\n",
    "\n",
    "print(\"Detected Language:\", language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googletrans==4.0.0-rc1  #install the library before importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wLwTF_trd5g",
    "outputId": "e4277910-7c78-4f1b-dd88-142448721fab"
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "\n",
    "text = \"This is a sample text in English.\"\n",
    "\n",
    "# Detect the language\n",
    "detected_lang = detect(text)\n",
    "\n",
    "# Translate to French\n",
    "translator = Translator()\n",
    "translated_text = translator.translate(text, src=detected_lang, dest='fr').text\n",
    "\n",
    "print(\"Detected Language:\", detected_lang)\n",
    "print(\"Translated Text (to French):\", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Python function using TextBlob to tokenize a given sentence and count the number of tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Python function using TextBlob to perform Parts of Speech (POS) tagging on a given sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Python function using TextBlob to perform spell checking on a given text and suggest corrections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Python function using langdetect and googletrans to perform trasnlation on a given text from english to chiense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiushqSmrd41"
   },
   "source": [
    "# <font color=\"maroon\"> Some other functions in NLP: Text Similarity Measures </font>\n",
    "\n",
    "- To measure distance between 2 string\n",
    "\n",
    "Applications\n",
    "- Information retrieval\n",
    "- Text classification\n",
    "- Document clustering\n",
    "- Topic Modeling\n",
    "- Matric decomposition\n",
    "\n",
    "To measure the word similarity, we use **<font color=\"blue\"><a href=\"https://pypi.org/project/python-Levenshtein/\" target=\"_blank\">Levenshtein distance</a></font>**.\n",
    "- Minimum number of operations to get from one word to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHCcSiv3rd41"
   },
   "source": [
    "![](https://i.imgur.com/FkdJmPi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bvYNWyGrd42",
    "outputId": "085ff014-8f55-4ab5-a669-118d4c0961ca"
   },
   "outputs": [],
   "source": [
    "pip install python-Levenshtein  #install before importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MlCBsGsrd42",
    "outputId": "bb7b14d9-3cc0-449a-c1d9-6ec9d6bc7dd5"
   },
   "outputs": [],
   "source": [
    "from Levenshtein import distance as lev\n",
    "lev('party', 'park')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svFS8eNIrd43",
    "outputId": "6b4c3382-458a-4c48-d9e2-1ab9073dd8b2"
   },
   "outputs": [],
   "source": [
    "#concept behind lev('party', 'park')\n",
    "def levenshtein_distance(s1, s2):\n",
    "    m, n = len(s1), len(s2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n",
    "\n",
    "    return dp[m][n]\n",
    "\n",
    "# Example usage\n",
    "string1 = \"party\"\n",
    "string2 = \"park\"\n",
    "distance = levenshtein_distance(string1, string2)\n",
    "print(\"Levenshtein distance:\", distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use the Levenshtein to measure the similarity between 2 sentences:\n",
    "<br>sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "<br>sentence2 = \"A quick brown fox jumps over a lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as lev\n",
    "\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "sentence2 = \"A quick brown fox jumps over a lazy dog.\"\n",
    "\n",
    "words1 = sentence1.lower().split()\n",
    "words2 = sentence2.lower().split()\n",
    "\n",
    "distance = lev(words1, words2)\n",
    "\n",
    "# Calculate similarity (adjust based on your specific needs)\n",
    "max_length = max(len(words1), len(words2))\n",
    "# print (max_length)\n",
    "similarity = 1 - (distance / max_length)\n",
    "\n",
    "print(\"Levenshtein distance between sentence 1 and sentence 2:\", distance)\n",
    "print(\"Similarity between sentence 1 and sentence 2:\", similarity)\n",
    "\n",
    "# However, it's important to note that Levenshtein distance is typically used for comparing sequences of characters, not entire sentences or phrases.\n",
    "# To measure similarity between sentences where the words are not necessarily in the same sequence, \n",
    "# you need to consider methods that can account for semantic similarity rather than just sequence-based similarity like Levenshtein distance.\n",
    "# Here are a few approaches you can explore: TF-IDF/Word Embeddings (pretrained model like Word2Vec, GloVe, or FastText) and Similarity Metrics (Cosine Similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zk2GVCZ-rd5g"
   },
   "source": [
    "# Text Format for Analysis: Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sD1MS75vrd5g",
    "outputId": "7e6267d6-c36e-4af9-ab46-227d873274e6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus =['This is the first document.', 'This is the second document.', 'And the third one. One is fun.'] #corpus=collection of teks\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus)\n",
    "pd.DataFrame(X.toarray(),columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdvU8fEVrd5h"
   },
   "source": [
    "![](https://i.imgur.com/OQDeQlb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zv94wmmjrd5h"
   },
   "source": [
    "# Document Similarity: Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYKH_Ntyrd5i"
   },
   "source": [
    "![](https://i.imgur.com/PyirXsy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLZ-ub-Yrd5j",
    "outputId": "055b92c6-0b5e-401c-fcba-a461a5c74d35"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['The weather is hot under the sun',\n",
    "'I make my hot chocolate with milk',\n",
    "'One hot encoding',\n",
    "'I will have a chai latte with milk',\n",
    "'There is a hot sale today']\n",
    "# create the document-term matrix with count vectorizer\n",
    "cv = CountVectorizer(stop_words=\"english\")\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "dt = pd.DataFrame(X, columns=cv.get_feature_names())\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Zzehizvrd5l"
   },
   "source": [
    "# Document Similarity: Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwo3s5KIrd5l",
    "outputId": "14825b2b-5357-40d0-aa96-a6714f98b90b"
   },
   "outputs": [],
   "source": [
    "# calculate the cosine similarity between all combinations of documents\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# list all of the combinations of 5 take 2 as well as the pairs of phrases\n",
    "pairs = list(combinations(range(len(corpus)),2)) #sentence (0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), .., (3,4))\n",
    "print(pairs)\n",
    "combos = [(corpus[a_index], corpus[b_index]) for (a_index, b_index) in pairs]\n",
    "print (combos)\n",
    "\n",
    "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
    "results = [cosine_similarity([X[a_index]], [X[b_index]]) for (a_index, b_index) in pairs]\n",
    "sorted(zip(results, combos), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6C9J5J45rd5m",
    "outputId": "eb844ee1-d829-4a39-c74d-db66a4988004"
   },
   "outputs": [],
   "source": [
    "pairs = list(combinations(range(5),2))\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYV4dhzfrd5m"
   },
   "source": [
    "![](https://i.imgur.com/jrfN6Jj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS0Stotjrd5m"
   },
   "source": [
    "![](https://i.imgur.com/BI8XP92.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_X6soeOrd5n"
   },
   "source": [
    "![](https://i.imgur.com/3IbfQXT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaTIJB99rd5n"
   },
   "source": [
    "![](https://i.imgur.com/pnNqzql.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBKAYd0erd5n",
    "outputId": "49b4b2e6-4181-4e1b-e3c3-c0d4e679993c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpus = ['This is the first document.',\n",
    "         'This is the second document.',\n",
    "         'And the third one. One is fun.']\n",
    "# original Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(X, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u44cBAImrd5o",
    "outputId": "626c48b7-172a-4fdc-9c62-cfd711a56ca3"
   },
   "outputs": [],
   "source": [
    "# new TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv_tfidf = TfidfVectorizer()\n",
    "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(X_tfidf, columns=cv_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEMme_lQrd5o"
   },
   "source": [
    "![](https://i.imgur.com/xlJibKw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIQbXx_Jrd5p"
   },
   "source": [
    "## Document Similarity: Example with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmQKsUwCrd5p",
    "outputId": "ad0f1871-a35f-42fe-8a4c-6f9b428c83ba"
   },
   "outputs": [],
   "source": [
    "corpus = ['The weather is hot under the sun',\n",
    "'I make my hot chocolate with milk',\n",
    "'One hot encoding',\n",
    "'I will have a chai latte with milk',\n",
    "'There is a hot sale today']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# create the document-term matrix with TF-IDF vectorizer\n",
    "cv_tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
    "dt_tfidf = pd.DataFrame(X_tfidf,columns=cv_tfidf.get_feature_names())\n",
    "dt_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QV41uOddrd5p",
    "outputId": "0fe1d7d3-c743-439f-889e-e8a0d1c247bf"
   },
   "outputs": [],
   "source": [
    "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
    "results_tfidf = [cosine_similarity([X_tfidf[a_index]], [X_tfidf[b_index]]) for (a_index, b_index) in pairs]\n",
    "sorted(zip(results_tfidf, combos), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StpSLAU1rd5q"
   },
   "source": [
    "![](https://i.imgur.com/mj4J60v.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
